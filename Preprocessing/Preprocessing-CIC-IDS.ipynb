{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-23-2018.csv 382840456\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "03-02-2018.csv 352368373\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-21-2018.csv 328893673\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-22-2018.csv 382636202\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-15-2018.csv 375945899\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-16-2018.csv 333723605\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-14-2018.csv 358223333\n",
      "chunck : 1 (1000000, 80)\n",
      "chunck : 2 (48575, 80)\n",
      "02-20-2018.csv 4054925350\n",
      "chunck : 1 (1000000, 84)\n",
      "chunck : 2 (1000000, 84)\n",
      "chunck : 3 (1000000, 84)\n",
      "chunck : 4 (1000000, 84)\n",
      "chunck : 5 (1000000, 84)\n",
      "chunck : 6 (1000000, 84)\n",
      "chunck : 7 (1000000, 84)\n",
      "chunck : 8 (948748, 84)\n",
      "02-28-2018.csv 209249758\n",
      "chunck : 1 (613104, 80)\n",
      "03-01-2018.csv 107842858\n",
      "chunck : 1 (331125, 80)\n"
     ]
    }
   ],
   "source": [
    "# Define the path of the folder of datas\n",
    "folder_path = '/Datasets/CIC_IDS2018/data'\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]    # Read all of dataset\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each CSV file and read it into a dataframe\n",
    "for file in csv_files:\n",
    "    df_name = os.path.splitext(file)[0]          # Get the datafeame name from te file name\n",
    "    file_path = os.path.join(folder_path, file)  # Get the full file path\n",
    "    print(file,os.path.getsize(file_path))\n",
    "    chunks=pd.read_table(file_path,chunksize=1000000,encoding='unicode_escape', sep=',', low_memory=False)\n",
    "    df=pd.DataFrame()\n",
    "    i=0\n",
    "    for chunk in chunks:\n",
    "         i+=1\n",
    "         print(\"chunck :\",i,chunk.shape)\n",
    "         df=pd.concat([df,chunk])\n",
    "    dfs[df_name] = df            # Store the dataframe in the dictionary with the same name as the file (e.g. dfs[data1] = df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape of 02-23-2018 = (1048575, 80)\n",
      "initial shape of 03-02-2018 = (1048575, 80)\n",
      "initial shape of 02-21-2018 = (1048575, 80)\n",
      "initial shape of 02-22-2018 = (1048575, 80)\n",
      "initial shape of 02-15-2018 = (1048575, 80)\n",
      "initial shape of 02-16-2018 = (1048575, 80)\n",
      "initial shape of 02-14-2018 = (1048575, 80)\n",
      "initial shape of 02-20-2018 = (7948748, 84)\n",
      "initial shape of 02-28-2018 = (613104, 80)\n",
      "initial shape of 03-01-2018 = (331125, 80)\n"
     ]
    }
   ],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    print(f'initial shape of {key} = {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Flow ID, Src IP and Dst IP columns of the 02-20-2018 dataset for being strings\n",
    "for item in dfs:\n",
    "    if item == '02-20-2018':\n",
    "        dfs['02-20-2018'].drop(['Flow ID','Src IP','Dst IP', 'Src Port'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]   # Get the dataframe corresponding to the key\n",
    "    # print(f\"Dataframe: '{key}'\\n\")\n",
    "\n",
    "    # replace +ve and -ve infinity with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Drop all NaN values\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "\n",
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    df['Timestamp'] = (df['Timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]\n",
    "    for col in df.columns:\n",
    "        #Check if the datatype of the column is object\n",
    "        if df[col].dtype == 'object' and col != 'Label':\n",
    "            # Change all values to numeric, and to NaN if it is a string\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    # print(f\"Dataframe: '{key}'\\n\")\n",
    "    count_NA = df.isna().sum()\n",
    "    # print(count_NA)\n",
    "\n",
    "\n",
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers Filtering:\n",
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "\n",
    "    # Define a function to filter outliers using Z-score\n",
    "    def filter_outliers_zscore(data, threshold):\n",
    "        z_scores = np.abs(stats.zscore(data))\n",
    "        outlier_mask = (z_scores > threshold).any(axis=1)\n",
    "        return data[~outlier_mask], data[outlier_mask]\n",
    "\n",
    "    # Define a threshold value\n",
    "    threshold = 7\n",
    "\n",
    "    # The filtering was removing all DDoS-LOIC-UDP, so we will not execute it on them\n",
    "    if key == '02-21-2018':\n",
    "        df_temp = df[df['Label'] == 'DDOS attack-LOIC-UDP']\n",
    "        df = df[df['Label'] != 'DDOS attack-LOIC-UDP']\n",
    "\n",
    "    # Loop through the columns of the dataframe and filter outliers in each column\n",
    "    filtered_cols = []\n",
    "    removed_outliers = []\n",
    "    for col in df.columns:\n",
    "        if col != 'Label':\n",
    "            filtered_col, outliers = filter_outliers_zscore(df[[col]], threshold)\n",
    "\n",
    "            filtered_cols.append(filtered_col)\n",
    "            removed_outliers.append(outliers)\n",
    "\n",
    "    # Combine the filtered columns back into a dataframe\n",
    "    df_filtered = pd.concat(filtered_cols, axis=1)\n",
    "\n",
    "    # Combine the removed outliers back into a dataframe\n",
    "    df_outliers = pd.concat(removed_outliers, axis=1)\n",
    "\n",
    "    n_outliers = df_outliers.shape[0]\n",
    " \n",
    "    # Assign filtered dataframe columns to original one\n",
    "    columns = [col for col in df.columns if col != 'Label']\n",
    "    df.loc[:,columns] = df_filtered.loc[:,columns]\n",
    "\n",
    "    # Recombine rows from 'DDOS attack-LOIC-UDP'\n",
    "    if key == '02-21-2018':\n",
    "        df = pd.concat([df,df_temp])\n",
    "\n",
    "    values_orig = df.loc[df.index.isin(df_outliers.index), 'Label']\n",
    "    # print(f'\\n{values_orig.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Correlation Filtering\n",
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "\n",
    "    columns = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "    corr_matrix = df[columns].corr().abs()\n",
    "\n",
    "    threshold = 0.99\n",
    "    # Find features with high correlation\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    # print(f'\\nDataset: {key}')\n",
    "    # Print features to drop\n",
    "    # print(f\"The following {len(to_drop)} features will be dropped due to high correlation: {to_drop}\")\n",
    "\n",
    "    df = df.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    # print(f\"Dataframe: '{key}'\\n\")\n",
    "    # replace +ve and -ve infinity with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Drop all NaN values\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dfs:\n",
    "\n",
    "    if item == '02-14-2018':\n",
    "        df = dfs['02-14-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df2 = df[df[\"Label\"] == \"FTP-BruteForce\"]\n",
    "        df3 = df[df[\"Label\"] == \"SSH-Bruteforce\"]\n",
    "        df0 = df[df[\"Label\"] == \"Benign\"][:df2.shape[0]]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df3.shape[0]]\n",
    "        df_equal_FTP_BruteForce = pd.concat([ df0,df2], axis =0)\n",
    "        df_equal_SSH_BruteForce = pd.concat([ df1,df3], axis =0)\n",
    "\n",
    "\n",
    "    if item == '02-15-2018':\n",
    "        df = dfs['02-15-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df2 = df[df[\"Label\"] == \"DoS attacks-GoldenEye\"]\n",
    "        df3 = df[df[\"Label\"] == \"DoS attacks-Slowloris\"]\n",
    "        df0 = df[df[\"Label\"] == \"Benign\"][:df2.shape[0]]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df3.shape[0]*100]\n",
    "        df_equal_DoS_GoldenEye = pd.concat([ df0,df2], axis =0)\n",
    "        df_equal_DoS_Slowloris = pd.concat([ df1,df3], axis =0)\n",
    "\n",
    "\n",
    "    if item == '02-16-2018':\n",
    "        df = dfs['02-16-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df3 = df[df[\"Label\"] == \"DoS attacks-SlowHTTPTest\"]\n",
    "        df0 = df[df[\"Label\"] == \"Benign\"]\n",
    "        df2 = df[df[\"Label\"] == \"DoS attacks-Hulk\"][:df0.shape[0]]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df3.shape[0]]\n",
    "        df_equal_DoS_Hulk = pd.concat([ df0,df2], axis =0)\n",
    "        df_equal_DoS_SlowHTTPTest = pd.concat([ df1,df3], axis =0)\n",
    "\n",
    "\n",
    "    if item == '02-21-2018':\n",
    "        df = dfs['02-21-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df3 = df[df[\"Label\"] == \"DDOS attack-LOIC-UDP\"]\n",
    "        df0 = df[df[\"Label\"] == \"Benign\"]\n",
    "        df2 = df[df[\"Label\"] == \"DDOS attack-HOIC\"][:df0.shape[0]]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df3.shape[0]*100]\n",
    "        df_equal_DDoS_HOIC = pd.concat([ df0,df2], axis =0)\n",
    "        df_equal_DDoS_LOIC_UDP = pd.concat([ df1,df3], axis =0)\n",
    "\n",
    "\n",
    "    if item == '02-28-2018' or item == '03-01-2018':\n",
    "        df_02_28 = dfs['02-28-2018']\n",
    "        df_03_01 = dfs['03-01-2018']\n",
    "        df_inf = pd.concat([ df_02_28,df_03_01], axis =0)\n",
    "        df_inf = df_inf.sample(frac=1) #Randomize rows's sequence\n",
    "        df2 = df_inf[df_inf[\"Label\"] == \"Infilteration\"]\n",
    "        df1 = df_inf[df_inf[\"Label\"] == \"Benign\"][:df2.shape[0]]\n",
    "        df_equal_Infilteration = pd.concat([ df1,df2], axis =0)\n",
    "\n",
    "\n",
    "    if item == '03-02-2018':\n",
    "        df = dfs['03-02-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df2 = df[df[\"Label\"] == \"Bot\"]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df2.shape[0]]\n",
    "        df_equal_Bot = pd.concat([ df1,df2], axis =0)\n",
    "\n",
    "\n",
    "    if item == '02-23-2018' or item == '02-22-2018':\n",
    "        df1 = dfs['02-23-2018']\n",
    "        df2 = dfs['02-22-2018']\n",
    "        df_BruteForce_Web_XSS = pd.concat([df1,df2], axis = 0)\n",
    "        df_BruteForce_Web_XSS[\"Label\"] = df_BruteForce_Web_XSS.Label.map(lambda a:\"Benign\" if a == 'Benign' else \"Attack\")\n",
    "\n",
    "\n",
    "    if item == '02-20-2018':\n",
    "        df = dfs['02-20-2018']\n",
    "        df = df.sample(frac=1) #Randomize rows's sequence\n",
    "        df2 = df[df[\"Label\"] == \"DDoS attacks-LOIC-HTTP\"]\n",
    "        df1 = df[df[\"Label\"] == \"Benign\"][:df2.shape[0]]\n",
    "        df_equal_DDoS_LOIC_HTTP = pd.concat([ df1,df2], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of 02-23-2018 = (1007064, 80)\n",
      "final shape of 03-02-2018 = (1009488, 80)\n",
      "final shape of 02-21-2018 = (1048575, 80)\n",
      "final shape of 02-22-2018 = (1011778, 80)\n",
      "final shape of 02-15-2018 = (992169, 80)\n",
      "final shape of 02-16-2018 = (1036539, 80)\n",
      "final shape of 02-14-2018 = (1011097, 80)\n",
      "final shape of 02-20-2018 = (7565845, 80)\n",
      "final shape of 02-28-2018 = (575450, 80)\n",
      "final shape of 03-01-2018 = (309531, 80)\n"
     ]
    }
   ],
   "source": [
    "# print shape of each dataset\n",
    "for key in dfs.keys():\n",
    "    df = dfs[key]  # Get the dataframe corresponding to the key\n",
    "    print(f'final shape of {key} = {df.shape}')\n",
    "\n",
    "\n",
    "dfs_final = {}\n",
    "\n",
    "for name, df in zip(['df_equal_Bot','df_equal_DDoS_HOIC',\n",
    "                         'df_equal_DDoS_LOIC_UDP','df_equal_DoS_GoldenEye','df_equal_DoS_Hulk',\n",
    "                         'df_equal_DoS_SlowHTTPTest','df_equal_DoS_Slowloris','df_equal_FTP_BruteForce',\n",
    "                         'df_equal_Infilteration','df_equal_SSH_BruteForce','df_BruteForce_Web_XSS',\n",
    "                         'df_equal_DDoS_LOIC_HTTP'],\n",
    "                         [df_equal_Bot,df_equal_DDoS_HOIC,\n",
    "                         df_equal_DDoS_LOIC_UDP,df_equal_DoS_GoldenEye,df_equal_DoS_Hulk,\n",
    "                         df_equal_DoS_SlowHTTPTest,df_equal_DoS_Slowloris,df_equal_FTP_BruteForce,\n",
    "                         df_equal_Infilteration,df_equal_SSH_BruteForce,df_BruteForce_Web_XSS,\n",
    "                         df_equal_DDoS_LOIC_HTTP]):\n",
    "    dfs_final[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign                      4836385\n",
      "DDoS attacks-LOIC-HTTP       573347\n",
      "DoS attacks-Hulk             439126\n",
      "DDOS attack-HOIC             360833\n",
      "Bot                          285763\n",
      "FTP-BruteForce               193354\n",
      "SSH-Bruteforce               187589\n",
      "Infilteration                152861\n",
      "DoS attacks-SlowHTTPTest     139890\n",
      "DoS attacks-GoldenEye         39924\n",
      "DoS attacks-Slowloris          2724\n",
      "DDOS attack-LOIC-UDP           1730\n",
      "Attack                          544\n",
      "Name: Label, dtype: int64\n",
      "shape of final dataset: (7214070, 80)\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat(dfs_final.values())\n",
    "print(combined['Label'].value_counts())\n",
    "print(f\"shape of final dataset: {combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined\n",
    "# Replace the values of \"Label\" with numerical values\n",
    "new_Label = df['Label'].replace({'Benign':0, 'DDoS attacks-LOIC-HTTP':1,\n",
    "                                 'DoS attacks-Hulk':2, 'DDOS attack-HOIC':1,\n",
    "                                 'Bot':3,'FTP-BruteForce':4, 'SSH-Bruteforce':4,\n",
    "                                 'Infilteration':5, 'DoS attacks-SlowHTTPTest':2,\n",
    "                                 'DoS attacks-GoldenEye':2, 'DoS attacks-Slowloris':2,\n",
    "                                 'DDOS attack-LOIC-UDP':1, 'Attack':6})\n",
    "df['Label'] = new_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the distribution of every class:\n",
    "to_drop_Benign = [0]\n",
    "to_drop_DDoS = [1]\n",
    "to_drop_DoS = [2]\n",
    "to_drop_Bot = [3]\n",
    "to_drop_BruteForce = [4]\n",
    "to_drop_Infilteration = [5]\n",
    "to_drop_Attack = [6]\n",
    "\n",
    "Benign_df = df[df['Label'].isin(to_drop_Benign)];\n",
    "DDoS_df = df[df['Label'].isin(to_drop_DDoS)];\n",
    "DoS_df = df[df['Label'].isin(to_drop_DoS)];\n",
    "Bot_df = df[df['Label'].isin(to_drop_Bot)];\n",
    "BruteForce_df = df[df['Label'].isin(to_drop_BruteForce)];\n",
    "Infilteration_df = df[df['Label'].isin(to_drop_Infilteration)];\n",
    "Attack_df = df[df['Label'].isin(to_drop_Attack)];\n",
    "\n",
    "\n",
    "# split dataset into labels and rest of it:\n",
    "X_Benign = Benign_df.drop('Label',1)\n",
    "Y_Benign = Benign_df.Label\n",
    "X_DDoS = DDoS_df.drop('Label',1)\n",
    "Y_DDoS = DDoS_df.Label\n",
    "X_DoS = DoS_df.drop('Label',1)\n",
    "Y_DoS = DoS_df.Label\n",
    "X_Bot = Bot_df.drop('Label',1)\n",
    "Y_Bot = Bot_df.Label\n",
    "X_BruteForce = BruteForce_df.drop('Label',1)\n",
    "Y_BruteForce = BruteForce_df.Label\n",
    "X_Infilteration = Infilteration_df.drop('Label',1)\n",
    "Y_Infilteration = Infilteration_df.Label\n",
    "X_Attack = Attack_df.drop('Label',1)\n",
    "Y_Attack = Attack_df.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "colNames=list(df)\n",
    "\n",
    "# Min_Max normalization:\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_Benign1 = scaler.fit_transform(X_Benign)\n",
    "X_DDoS1 = scaler.fit_transform(X_DDoS)\n",
    "X_DoS1 = scaler.fit_transform(X_DoS)\n",
    "X_Bot1 = scaler.fit_transform(X_Bot)\n",
    "X_BruteForce1 = scaler.fit_transform(X_BruteForce)\n",
    "X_Infilteration1 = scaler.fit_transform(X_Infilteration)\n",
    "X_Attack1 = scaler.fit_transform(X_Attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7214070, 80)\n",
      "(7214070, 80)\n"
     ]
    }
   ],
   "source": [
    "# Add the column of label into normalized datset\n",
    "Benign = np.hstack((X_Benign1, Y_Benign[:, np.newaxis]))\n",
    "DDoS = np.hstack((X_DDoS1, Y_DDoS[:, np.newaxis]))\n",
    "DoS = np.hstack((X_DoS1, Y_DoS[:, np.newaxis]))\n",
    "Bot = np.hstack((X_Bot1, Y_Bot[:, np.newaxis]))\n",
    "BruteForce = np.hstack((X_BruteForce1, Y_BruteForce[:, np.newaxis]))\n",
    "Infilteration = np.hstack((X_Infilteration1, Y_Infilteration[:, np.newaxis]))\n",
    "Attack = np.hstack((X_Attack1, Y_Attack[:, np.newaxis]))\n",
    "\n",
    "np_data = np.concatenate((Benign, DDoS, DoS, Bot, BruteForce, Infilteration, Attack))\n",
    "np.random.shuffle(np_data)\n",
    "print(np_data.shape)\n",
    "\n",
    "df1 = pd.DataFrame(np_data, columns=colNames)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    4836385\n",
      "1.0     935910\n",
      "2.0     621664\n",
      "4.0     380943\n",
      "3.0     285763\n",
      "5.0     152861\n",
      "6.0        544\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/Datasets/CIC_IDS2018/'\n",
    "filename = os.path.join(output_dir, 'normalized_CIC_IDS' + '.csv')\n",
    "df1.to_csv(filename, index = False)\n",
    "print(df1['Label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
